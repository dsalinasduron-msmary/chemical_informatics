{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > new_lambda_requirements.txt\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from flair.data import Sentence\n",
    "#from flair.nn import Classifier\n",
    "#from chembl_webresource_client.new_client import new_client\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "insert code here that\n",
    "1. Takes an EFO as input\n",
    "2. displays a dataframe of targets for that disease\n",
    "3. uploads that dataframe to a table in planetscale called \"disease_to_target\"\n",
    "\n",
    "\"\"\"\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import dotenv_values\n",
    "from sqlalchemy import text\n",
    "config = dotenv_values('database_url.env')\n",
    "url = config['DATABASE_URL']\n",
    "\n",
    "engine = create_engine(url, echo=False)\n",
    "\n",
    "\n",
    "\n",
    "## Set disease_id variable for desired disease\n",
    "\n",
    "disease_id = \"EFO_0005537\"\n",
    "\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    query = text(\"SELECT * FROM disease_to_target WHERE disease_id = '{disease_id}';\".format(disease_id = disease_id))\n",
    "    disease_df = pd.read_sql(query, conn)\n",
    "\n",
    "display(disease_df)\n",
    "\n",
    "\n",
    "target_ids = disease_df.sort_values(by = ['association_score'], ascending=False)['target_ensemble_id'].values\n",
    "print(target_ids)\n",
    "target_id = target_ids[1]\n",
    "\n",
    "print(target_id)\n",
    "print(type(target_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "insert code here that\n",
    "1. takes a target as input\n",
    "2. searches the database for compounds and assays for that target\n",
    "3. displays a dataframe with at least these columns: target id, compound id, assay id\n",
    "\"\"\"\n",
    "engine = create_engine(url, echo=False)\n",
    "#target_id = 'CHEMBL4776444'\n",
    "\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    query = text(\"SELECT * FROM target_to_compounds WHERE target_ensemble_id='{target}';\".format(target=target_id))\n",
    "    target_to_compounds_df = pd.read_sql(query, conn)\n",
    "\n",
    "display(target_to_compounds_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to tell deepchem where to find cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre deepchem[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "insert code here that\n",
    "1. trains a model to predict ic50 values for a compound on a given target\n",
    "2. downloads ALL the compounds in the database (independent of target)\n",
    "3. runs the model to predict ic50 values for each compound in the database\n",
    "4. displays a dataframe that has the following two columns : compound, predicted ic50\n",
    "5. prints out the compound that has the highest ic50 value that has NOT been tested on the target already.\n",
    "\"\"\"\n",
    "# 1. Train Model\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "#!git clone https://github.com/dsalinasduron-msmary/chemical_informatics.git\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "#os.listdir()\n",
    "#os.chdir('chemical_informatics')\n",
    "#os.getcwd()\n",
    "\"\"\"\n",
    "f = open('ENSG00000128191dataframe.pickle','rb')\n",
    "compound_dataset = pickle.load(f)\n",
    "f.close()\n",
    "display(compound_dataset)\n",
    "\"\"\"\n",
    "compound_dataset = target_to_compounds_df\n",
    "smiles = compound_dataset['smiles']\n",
    "IC50 = compound_dataset['standard_value']\n",
    "featurizer = dc.feat.ConvMolFeaturizer()\n",
    "compound_dataset['featurized'] = featurizer.featurize(smiles)\n",
    "compound_dataset['divided values'] = compound_dataset['standard_value'].astype(float).div(108000)\n",
    "compound_dataset['pIC50'] = np.log10(compound_dataset['divided values'].astype(float)).mul(-1)\n",
    "compound_dataset['number'] = list(range(0,len(compound_dataset)))\n",
    "display(compound_dataset)\n",
    "\n",
    "#compound_dataset['pIC50'].hist()\n",
    "\"\"\"\n",
    "x = compound_dataset\n",
    "f = open('x.pickle','wb')\n",
    "pickle.dump(x,f)\n",
    "f.close()\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "training_dataset = compound_dataset.sample(frac = 0.7)\n",
    "\n",
    "#training_dataset.featurized[0].n_feat\n",
    "\n",
    "testing_dataset = (compound_dataset[~compound_dataset['number'].isin(training_dataset['number'])])\n",
    "display(testing_dataset)\n",
    "\n",
    "numpy_training_dataset = dc.data.NumpyDataset(X=training_dataset['featurized'],y=training_dataset['pIC50'].astype(float), ids=training_dataset['smiles'])\n",
    "numpy_testing_dataset = dc.data.NumpyDataset(X=testing_dataset['featurized'],y=testing_dataset['pIC50'].astype(float), ids=testing_dataset['smiles'])\n",
    "display(numpy_training_dataset)\n",
    "display(numpy_testing_dataset)\n",
    "\n",
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2, dense_layer_size=10)\n",
    "\n",
    "model.fit(numpy_training_dataset, nb_epoch=100)\n",
    "\n",
    "#model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2, dense_layer_size=10, model_dir=\"./compund_model\")\n",
    "#model.restore()\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print(\"Training set score:\", model.evaluate(numpy_training_dataset, [metric]))\n",
    "print(\"Test set score:\", model.evaluate(numpy_testing_dataset, [metric]))\n",
    "\n",
    "\n",
    "#2. Download all compounds in database\n",
    "with engine.begin() as conn:\n",
    "    query = text(\"SELECT compound_id,smiles FROM target_to_compounds;\")\n",
    "    compounds_df = pd.read_sql(query, conn)\n",
    "\n",
    "predict_list = []\n",
    "for smile in range(0, len(compounds_df)):\n",
    "    predict_list.append(None)\n",
    "print(len(predict_list))\n",
    "\n",
    "compounds_df['predicted_pIC50'] = predict_list\n",
    "\n",
    "curated_compounds_df = compounds_df.dropna(subset=['smiles'])\n",
    "\n",
    "new_smiles = curated_compounds_df['smiles']\n",
    "curated_compounds_df['featurized'] = featurizer.featurize(new_smiles)\n",
    "\n",
    "#3. run the model to predict ic50 values for each compound in the database\n",
    "dataset = dc.data.NumpyDataset(X=curated_compounds_df['featurized'], y=curated_compounds_df['predicted_pIC50'], ids=curated_compounds_df['smiles'])\n",
    "curated_compounds_df['predicted_pIC50'] = model.predict(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# 4. displays a dataframe that has the following two columns : compound, predicted ic50\n",
    "display(curated_compounds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. print out the compound that has the highest ic50 value that has NOT been tested on the target already.\n",
    "\n",
    "curated_compounds_no_target = (curated_compounds_df[~curated_compounds_df['smiles'].isin(target_to_compounds_df['smiles'])])\n",
    "\n",
    "\n",
    "\n",
    "df = curated_compounds_no_target.sort_values(by = ['predicted_pIC50'], ascending=False)\n",
    "df.drop_duplicates(subset = 'smiles', inplace = True)\n",
    "\n",
    "display(df)\n",
    "compound_ids = df['compound_id'].values\n",
    "compound_smiles = df['smiles'].values\n",
    "\n",
    "compound_ids_and_smiles = list(zip(compound_ids, compound_smiles))\n",
    "best_compound = (compound_ids_and_smiles[0])\n",
    "\n",
    "# 6. get InChI key for top compound and generate a url for Zinc\n",
    "\n",
    "url = \"https://cactus.nci.nih.gov/chemical/structure/{smiles}/stdinchikey\".format(smiles = best_compound[1])\n",
    "r = requests.get(url=url)\n",
    "raw_inchikey = r.text\n",
    "inchikey = raw_inchikey.split('=')[1]\n",
    "\n",
    "print(inchikey)\n",
    "\n",
    "zinc_url = 'https://zinc15.docking.org/substances/?inchikey={inchikey}'.format(inchikey = inchikey)\n",
    "print(zinc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw, PyMol, rdFMCS\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import rdBase\n",
    "from deepchem import metrics\n",
    "from IPython.display import Image, display\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.head(10)\n",
    "dataset = dc.data.NumpyDataset(X=small_df['featurized'],y=small_df['predicted_pIC50'].astype(float), ids=small_df['smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compound_dataset['smi_to_mol'] = Chem.SmilesMolSupplierFromText(compound_dataset['smiles'].values.tolist())\n",
    "#pd.DataFrame.to_csv(compound_dataset)\n",
    "just_smiles_df = pd.DataFrame()\n",
    "#just_smiles_df['smiles'] = compound_dataset['smiles']\n",
    "just_smiles_df['smiles'] = small_df['smiles']\n",
    "smiles = just_smiles_df['smiles'].tolist\n",
    "small_smiles = just_smiles_df.head(10)\n",
    "#just_smiles_df['name'] = [-1 for thing in smiles]\n",
    "just_smiles_df['name'] = just_smiles_df['smiles']\n",
    "#print(just_smiles_df)\n",
    "just_smiles_df.to_csv('smiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = small_df['smiles']\n",
    "featurizer = dc.feat.ConvMolFeaturizer(per_atom_fragmentation = True)\n",
    "small_df['frag_featurized'] = featurizer.featurize(smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_dataset = dc.data.NumpyDataset(X=small_df['frag_featurized'], y = None, w = None, ids = dataset.ids)\n",
    "print(frag_dataset.get_shape)\n",
    "tr = dc.trans.FlatteningTransformer(frag_dataset) # flatten dataset and add ids to each fragment\n",
    "frag_dataset = tr.transform(frag_dataset)\n",
    "print(frag_dataset.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole molecules\n",
    "pred = model.predict(dataset)\n",
    "pred = pd.DataFrame(pred, index=dataset.ids, columns=[\"Molecule\"])  # turn to dataframe for convenience\n",
    "display(pred)\n",
    "# fragments\n",
    "pred_frags = model.predict(frag_dataset)\n",
    "pred_frags = pd.DataFrame(pred_frags, index=frag_dataset.ids, columns=[\"Fragment\"])  # turn to dataframe for convenience\n",
    "#pred_frags = pd.DataFrame(pred_frags,index=range(0, len(frag_dataset)), columns=[\"Fragment\"])\n",
    "print(pred_frags)\n",
    "# merge 2 dataframes by molecule names\n",
    "mol_df = pd.merge(pred_frags, pred, right_index=True, left_index=True)\n",
    "# find contribs\n",
    "mol_df['Contrib'] = mol_df[\"Molecule\"] - mol_df[\"Fragment\"]\n",
    "display(mol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_contribs(mols, df, smi_or_sdf = \"smi\"): \n",
    "    # input format of file, which was used to create dataset determines the order of atoms, \n",
    "    # so we take it into account for correct mapping!\n",
    "    maps = []\n",
    "    for mol  in mols:\n",
    "        wt = {}\n",
    "        if smi_or_sdf == \"smi\":\n",
    "            print(mol)\n",
    "            for n,atom in enumerate(Chem.rdmolfiles.CanonicalRankAtoms(mol)):\n",
    "                wt[atom] = df.loc[mol.GetProp(\"_Name\"),\"Contrib\"][n]\n",
    "\n",
    "        if smi_or_sdf == \"sdf\":        \n",
    "            for n,atom in enumerate(range(mol.GetNumHeavyAtoms())):\n",
    "                wt[atom] = df.loc[Chem.MolToSmiles(mol),\"Contrib\"][n]\n",
    "        maps.append(SimilarityMaps.GetSimilarityMapFromWeights(mol,wt))\n",
    "    return maps    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [m for m in Chem.SmilesMolSupplier('smiles.csv', ',') if m is not None]\n",
    "#mols = [Chem.MolFromSmiles(m) for m in small_df['smiles'] if m is not None]\n",
    "print(mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_contribs(mols, mol_df, 'smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model_dir)\n",
    "print(dir(model))\n",
    "model.save_checkpoint()\n",
    "\"\"\"\n",
    "f = open('model.pickle', 'wb')\n",
    "pickle.dump(model, f)\n",
    "f.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_compound = (compound_ids_and_smiles[4])\n",
    "\n",
    "url = \"https://cactus.nci.nih.gov/chemical/structure/{smiles}/stdinchikey\".format(smiles = best_compound[1])\n",
    "r = requests.get(url=url)\n",
    "raw_inchikey = r.text\n",
    "inchikey = raw_inchikey.split('=')[1]\n",
    "\n",
    "print(inchikey)\n",
    "\n",
    "zinc_url = 'https://zinc15.docking.org/substances/?inchikey={inchikey}'.format(inchikey = inchikey)\n",
    "print(zinc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "from matplotlib.pyplot import hist\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "insert code here that\n",
    "1. retrieves the terms for all the assays that are relevant to the target the user picked.\n",
    "2. clusters the assays according to their descriptive terms\n",
    "3. plots the clusters (set n_clusters = 10)\n",
    "4. prints out the title of one assay from each cluster.\n",
    "\"\"\"\n",
    "\n",
    "config = dotenv_values(\"database_url.env\")\n",
    "url = config['DATABASE_URL']\n",
    "engine = create_engine(url, echo=False)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    query = text(\"select * from target_to_compounds;\")\n",
    "    target_list = pd.read_sql(query, conn)\n",
    "display(target_list)\n",
    "\n",
    "\n",
    "three_col_list = target_list[['assay_id', 'assay_description', 'abstract']].copy()\n",
    "\n",
    "unique_list = []\n",
    "for index, row in three_col_list.iterrows():\n",
    "    if (row[\"assay_id\"], row[\"assay_description\"], row[\"abstract\"]) in unique_list:\n",
    "        continue\n",
    "    else:\n",
    "        unique_list.append((row[\"assay_id\"], row[\"assay_description\"], row[\"abstract\"]))\n",
    "\n",
    "Assay_Descriptions = [abstract for (assay_ids, assay_name, abstract) in unique_list]\n",
    "Assay_Descriptions_Joined = ':: '.join(Assay_Descriptions)\n",
    "\n",
    "print(len(unique_list))\n",
    "#print(unique_list)\n",
    "#print(Assay_Descriptions)\n",
    "\n",
    "\n",
    "Assay_Descriptions_List = Assay_Descriptions\n",
    "\n",
    "\n",
    "Assay_Count_Vect = CountVectorizer()\n",
    "Assay_Train_Counts = Assay_Count_Vect.fit_transform(Assay_Descriptions_List)\n",
    "n_clusters = 20\n",
    "X = Assay_Train_Counts.toarray()\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(Assay_Train_Counts)\n",
    "tf_idf_vector = tfidf_transformer.transform(Assay_Train_Counts)\n",
    "\n",
    "\n",
    "ward = AgglomerativeClustering(\n",
    "    n_clusters = n_clusters, linkage=\"ward\", connectivity=None, compute_full_tree= True,compute_distances= True\n",
    ")\n",
    "ward.fit(X)\n",
    "\n",
    "\n",
    "\n",
    "unique_labels, counts = np.unique(ward.labels_, return_counts=True)\n",
    "cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "largest_cluster_label = max(cluster_sizes, key=cluster_sizes.get)\n",
    "largest_cluster_size = cluster_sizes[largest_cluster_label]\n",
    "\n",
    "print(f\"largest cluster: Cluster {largest_cluster_label}, Size: {largest_cluster_size}\")\n",
    "\n",
    "cluster_labels = ward.fit_predict(tf_idf_vector.toarray())\n",
    "\n",
    "\n",
    "\n",
    "cluster_assay_descriptions = {}\n",
    "for cluster_label, assay_description in zip(cluster_labels, Assay_Descriptions):\n",
    "    if cluster_label not in cluster_assay_descriptions:\n",
    "        cluster_assay_descriptions[cluster_label] = []\n",
    "    cluster_assay_descriptions[cluster_label].append(assay_description)\n",
    "    \n",
    "\n",
    "\n",
    "sorted_clusters = sorted(cluster_assay_descriptions.keys())\n",
    "\n",
    "for cluster_label in sorted_clusters:\n",
    "    assay_descriptions = cluster_assay_descriptions[cluster_label]\n",
    "    print(f\"Cluster {cluster_label}: {assay_descriptions[0]}\")\n",
    "\n",
    "print(ward.distances_)\n",
    "print(ward.distances_.shape)\n",
    "\n",
    "X_Hist = ward.distances_\n",
    "kernel = stats.gaussian_kde(X_Hist)\n",
    "print(kernel(X_Hist))\n",
    "\n",
    "seaborn.clustermap(X,method='ward')\n",
    "\n",
    "\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_,index=Assay_Count_Vect.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "count_vector=Assay_Count_Vect.transform(Assay_Descriptions_List)\n",
    "\n",
    "\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "print(tf_idf_vector)\n",
    "tf_idf_vector.shape\n",
    "\n",
    "feature_names = Assay_Count_Vect.get_feature_names_out()\n",
    "\n",
    "first_document_vector=tf_idf_vector[0]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "seaborn.clustermap(tf_idf_vector.toarray(),method='ward')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tagger = Classifier.load('hunflair')\n",
    "print(\"Finished setting tagger\")\n",
    "\n",
    "Assay_Descriptions_List = []\n",
    "\n",
    "sentence = Sentence(Assay_Descriptions_Joined)\n",
    "tagger.predict(sentence)\n",
    "\n",
    "Assay_Key_Words = \"\"\n",
    "for label in sentence.get_labels():\n",
    "    Assay_Key_Words += label.data_point.text\n",
    "    Assay_Key_Words += \" \"\n",
    "    #print(label.data_point.text)\n",
    "    #print(Assay_Key_Words)\n",
    "\n",
    "#adds the Descriptions of the Assays to the Assay Description List\n",
    "Assay_Descriptions_List.append(Assay_Key_Words)\n",
    "\n",
    "def tokenize_with_flair(description):\n",
    "    sentence = Sentence(description)\n",
    "    tagger.predict(sentence)\n",
    "    return [ l.data_point.text for l in sentence.get_labels() ]\n",
    "\n",
    "\n",
    "\n",
    "Assay_Count_Vect = CountVectorizer(tokenizer=tokenize_with_flair)\n",
    "Assay_Train_Counts = Assay_Count_Vect.fit_transform(Assay_Descriptions_List)\n",
    "\n",
    "n_clusters = 10  # number of regions\n",
    "X = Assay_Train_Counts.toarray()\n",
    "\n",
    "ward = AgglomerativeClustering(\n",
    "    n_clusters = n_clusters, linkage=\"ward\", connectivity=None, compute_full_tree= True,compute_distances= True\n",
    ")\n",
    "ward.fit(X)\n",
    "\n",
    "print(ward.distances_)\n",
    "print(ward.distances_.shape)\n",
    "\n",
    "X_Hist = ward.distances_\n",
    "#hist(X_Hist)\n",
    "kernel = stats.gaussian_kde(X_Hist)\n",
    "print(kernel(X_Hist))\n",
    "\n",
    "#first dendogram\n",
    "seaborn.clustermap(X,method='ward')\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(Assay_Train_Counts)\n",
    "\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_,index=Assay_Count_Vect.get_feature_names(),columns=[\"idf_weights\"])\n",
    "\n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# count matrix\n",
    "count_vector=Assay_Count_Vect.transform(Assay_Descriptions_List)\n",
    "\n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "print(tf_idf_vector)\n",
    "tf_idf_vector.shape\n",
    "\n",
    "feature_names = Assay_Count_Vect.get_feature_names()\n",
    "\n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    "\n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "\n",
    "#second dendogram\n",
    "seaborn.clustermap(tf_idf_vector.toarray(),method='ward')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cluster_labels))\n",
    "print(len(Assay_Descriptions))\n",
    "print(cluster_assay_descriptions[4][0] == cluster_assay_descriptions[4][1])\n",
    "print(cluster_assay_descriptions[4][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chembl_webresource_client.new_client import new_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
